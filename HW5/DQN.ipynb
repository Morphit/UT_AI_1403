{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b04a04d0-0bc1-4b01-9a9a-77c60e7352d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "# Defining an environment:\n",
    "    - 4 DOF robotic arm with arbitrary link lengths\n",
    "    - Target\n",
    "    - Obstacles\n",
    "'''\n",
    "class MultiArmRoboticEnv:\n",
    "    def __init__(self, num_links=4, target=(1.0, 1.0, 1.0)):\n",
    "        self.num_links = num_links\n",
    "        self.link_length = [0.5, 0.8, 0.8, 0.5]\n",
    "        self.angles = [0,0,0,0]\n",
    "        self.target = target\n",
    "        self.obstacles = [(0.5, 0.5, 0.5), (1.5, 1.5, 1.5)]\n",
    "        self.steps = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.angles = [0,0,0,0]\n",
    "        self.steps = 0\n",
    "        return self.angles\n",
    "\n",
    "    def get_end_effector(self, arm_index):\n",
    "        x = 0\n",
    "        y = 0\n",
    "        z = 0\n",
    "        if arm_index == 1:\n",
    "            x = 0\n",
    "            y = 0\n",
    "            z = self.link_length[0]\n",
    "        elif arm_index == 2:\n",
    "            x = np.cos(self.angles[0]) * (self.link_length[1] * np.cos(self.angles[1]))\n",
    "            y = np.sin(self.angles[0]) * (self.link_length[1] * np.cos(self.angles[1]))\n",
    "            z = self.link_length[0] + self.link_length[1] * np.sin(self.angles[1])\n",
    "        elif arm_index == 3:\n",
    "            x = np.cos(self.angles[0]) * \\\n",
    "            (self.link_length[1] * np.cos(self.angles[1]) + self.link_length[2] * np.cos(self.angles[1] + self.angles[2]))\n",
    "            y = np.sin(self.angles[0]) * \\\n",
    "            (self.link_length[1] * np.cos(self.angles[1]) + self.link_length[2] * np.cos(self.angles[1] + self.angles[2]))\n",
    "            z = self.link_length[0] + self.link_length[1] * np.sin(self.angles[1]) + self.link_length[2] * np.sin(self.angles[1] + self.angles[2]) \n",
    "        elif arm_index == 4:\n",
    "            x = np.cos(self.angles[0]) * (self.link_length[1] * np.cos(self.angles[1]) + self.link_length[2] * np.cos(self.angles[1] + self.angles[2]) + self.link_length[3] * np.cos(self.angles[1] + self.angles[2] + self.angles[3]) ) \n",
    "            y = np.sin(self.angles[0]) * (self.link_length[1] * np.cos(self.angles[1]) + self.link_length[2] * np.cos(self.angles[1] + self.angles[2]) + self.link_length[3] * np.cos(self.angles[1] + self.angles[2] + self.angles[3])) \n",
    "            z = self.link_length[0] + self.link_length[1] * np.sin(self.angles[1]) + self.link_length[2] * np.sin(self.angles[1] + self.angles[2]) + self.link_length[3] * np.sin(self.angles[1] + self.angles[2] + self.angles[3]) \n",
    "        return (x, y, z)\n",
    "\n",
    "    def step(self, actions):\n",
    "        action_map = {0: -0.1, 1: 0.1}\n",
    "        rewards = 0\n",
    "        self.steps += 1\n",
    "\n",
    "        for j in range(len(self.angles)):\n",
    "            end_effector = self.get_end_effector(self.num_links-1)\n",
    "            distance_old = np.linalg.norm(np.array(end_effector) - np.array(self.target))\n",
    "            self.angles[j] = np.clip(self.angles[j] + action_map[actions[j]], -np.pi, np.pi)\n",
    "            end_effector = self.get_end_effector(self.num_links-1)\n",
    "            distance_new = np.linalg.norm(np.array(end_effector) - np.array(self.target))\n",
    "            \n",
    "            if distance_old > distance_new:\n",
    "                rewards += 1\n",
    "            else:\n",
    "                rewards -= 1\n",
    "                \n",
    "            for obs in self.obstacles:\n",
    "                if np.linalg.norm(np.array(end_effector) - np.array(obs)) < 0.2:\n",
    "                    rewards -= 2000           \n",
    "        \n",
    "        # rewards -= 1  # Penalize based on the number of steps\n",
    "        # rewards -= 0.1 * self.steps\n",
    "        done = False\n",
    "        if np.linalg.norm(np.array(self.get_end_effector(self.num_links)) - np.array(self.target)) < 0.1:\n",
    "            rewards += 50000;\n",
    "            done = True\n",
    "            \n",
    "        return self.angles, rewards, done\n",
    "\n",
    "    def render(self):\n",
    "        for i in range(self.num_links):\n",
    "            print(f\"Link {i+1} Joint Angle: {self.angles[i]}\")\n",
    "        end_effector = self.get_end_effector(self.num_links)\n",
    "        print(f\"End Effector: {end_effector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fbfa941-3a21-4fcd-a996-77d8052c2b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def draw_robot(points, target):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(0, 0, 0, color='blue', s=100)\n",
    "    \n",
    "    for i in range(len(points)):\n",
    "        if i==len(points)-1:\n",
    "            ax.scatter(points[i][0], points[i][1], points[i][2], color='red', s=100)\n",
    "        else:\n",
    "            ax.scatter(points[i][0], points[i][1], points[i][2], color='blue', s=100)\n",
    "\n",
    "    for i in range(len(points)-1):\n",
    "        ax.plot([points[i][0], points[i+1][0]], [points[i][1], points[i+1][1]], [points[i][2], points[i+1][2]], color='black')\n",
    "    ax.plot([0, points[0][0]], [0, points[0][1]], [0, points[0][2]], color='black')\n",
    "\n",
    "    ax.scatter(target[0], target[1], target[2], color='green', s=50)\n",
    "    \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "\n",
    "    ax.set_xlim([-2,2])\n",
    "    ax.set_ylim([-2,2])\n",
    "    ax.set_zlim([0,3])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9cdb7e5-62c1-4657-b8c3-89d88c723977",
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 113\u001b[0m\n\u001b[0;32m    111\u001b[0m action_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    112\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(env, state_size, action_size)\n\u001b[1;32m--> 113\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m    115\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[1;32mIn[39], line 92\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self, episodes, target_update)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# while not done:\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# print(t)\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[0;32m     93\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n",
      "Cell \u001b[1;32mIn[39], line 54\u001b[0m, in \u001b[0;36mDQNAgent.choose_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     53\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_network(state)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(q_values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1242\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define the Q-Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Define the DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, state_size, action_size, gamma=0.99, lr=0.001, batch_size=64, epsilon=0.2, epsilon_decay=0.995, min_epsilon=0.01, memory_size=10000):\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.q_network = QNetwork(state_size, action_size).to(self.device)\n",
    "        self.target_network = QNetwork(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return [random.choice([0, 1]) for _ in range(self.env.num_links)]\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state).cpu().numpy()\n",
    "            return np.argmax(q_values, axis=1).tolist()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        q_values = self.q_network(states)\n",
    "        next_q_values = self.target_network(next_states)\n",
    "        q_target = q_values.clone()\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(self.env.num_links):\n",
    "                q_target[i][actions[i][j]] = rewards[i] + (1 - dones[i]) * self.gamma * torch.max(next_q_values[i])\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, episodes=1000, target_update=10):\n",
    "        for episode in range(episodes):\n",
    "            # print(1)\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            # while not done:\n",
    "            for t in range (100):\n",
    "                # print(t)\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                self.replay()\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            if episode % target_update == 0:\n",
    "                self.update_target_network()\n",
    "            print(f\"Episode {episode+1}/{episodes}, Total Reward: {total_reward}, Epsilon: {self.epsilon}\")\n",
    "            env.render()\n",
    "            draw_robot([env.get_end_effector(1),env.get_end_effector(2),env.get_end_effector(3),env.get_end_effector(4)], env.target)\n",
    "\n",
    "env = MultiArmRoboticEnv()\n",
    "state_size = env.num_links\n",
    "action_size = 2\n",
    "agent = DQNAgent(env, state_size, action_size)\n",
    "agent.train(episodes=1000)\n",
    "\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a002e8-f35f-42ef-a6d8-1f51aabebf02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define the Q-Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, num_links):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.num_links = num_links\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size * num_links)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x).view(-1, self.num_links, 2)\n",
    "\n",
    "# Define the DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, state_size, action_size, gamma=0.99, lr=0.001, batch_size=64, epsilon=0.3, epsilon_decay=0.995, min_epsilon=0.01, memory_size=10000):\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.q_network = QNetwork(state_size, action_size, env.num_links).to(self.device)\n",
    "        self.target_network = QNetwork(state_size, action_size, env.num_links).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return [random.choice([0, 1]) for _ in range(self.env.num_links)]\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state).cpu().numpy()\n",
    "            return [np.argmax(q_values[0][i]) for i in range(self.env.num_links)]\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        q_values = self.q_network(states)\n",
    "        next_q_values = self.target_network(next_states)\n",
    "        \n",
    "        q_target = q_values.clone()\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(self.env.num_links):\n",
    "                q_target[i][j][actions[i][j]] = rewards[i] + (1 - dones[i]) * self.gamma * torch.max(next_q_values[i][j])\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, episodes=1000, target_update=10):\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            # while not done:\n",
    "            for t in range (1000):\n",
    "                # print(t)\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                self.replay()\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            if episode % target_update == 0:\n",
    "                self.update_target_network()\n",
    "\n",
    "            print(f\"Episode {episode+1}/{episodes}, Total Reward: {total_reward}, Epsilon: {self.epsilon}\")\n",
    "            env.render()\n",
    "            draw_robot([env.get_end_effector(1),env.get_end_effector(2),env.get_end_effector(3),env.get_end_effector(4)], env.target)\n",
    "\n",
    "env = MultiArmRoboticEnv()\n",
    "state_size = env.num_links\n",
    "action_size = 2  # Increase or decrease the joint angle\n",
    "agent = DQNAgent(env, state_size, action_size)\n",
    "agent.train(episodes=1000)\n",
    "\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50361a89-7d12-4571-9ecc-db19ebeedea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
